# AnÃ¡lise de nLLMs - one-llm-4-all

**Data:** 02/02/2026 17:35  
**Objetivo:** Identificar e documentar todas as implementaÃ§Ãµes nativas de LLMs (nLLMs) presentes no sistema

---

## ğŸ“Š Resumo Executivo

O sistema **one-llm-4-all** possui **3 (trÃªs) implementaÃ§Ãµes nativas de LLMs** (nLLMs), todas localizadas na pasta `/packages`:

1. **OpenAI Client** (`/packages/openai`)
2. **Anthropic Client** (`/packages/anthropic`)
3. **Google GenAI Client** (`/packages/google-genai`)

---

## ğŸ” Detalhamento das nLLMs

### 1. OpenAI Client (`/packages/openai/index.ts`)

**DescriÃ§Ã£o:** ImplementaÃ§Ã£o nativa compatÃ­vel com a API da OpenAI.

**CaracterÃ­sticas:**
- âœ… Suporte a streaming
- âœ… Formato JSON (`response_format: { type: 'json_object' }`)
- âœ… ConfiguraÃ§Ã£o de `baseURL` customizÃ¡vel
- âœ… ParÃ¢metros: `temperature`, `max_tokens`, `top_p`, `stop`

**Provedores suportados via compatibilidade OpenAI:**
- OpenAI (`https://api.openai.com/v1`)
- Groq (`https://api.groq.com/openai/v1`)
- OpenRouter (`https://openrouter.ai/api/v1`)
- DeepSeek (`https://api.deepseek.com`)
- Mistral AI (`https://api.mistral.ai/v1`)
- Perplexity (`https://api.perplexity.ai`)

**MÃ©todos principais:**
```typescript
async createChatCompletion(params: ChatCompletionParams)
private async createChatCompletionStream(params: ChatCompletionParams)
```

---

### 2. Anthropic Client (`/packages/anthropic/index.ts`)

**DescriÃ§Ã£o:** ImplementaÃ§Ã£o nativa para Claude (Anthropic).

**CaracterÃ­sticas:**
- âœ… Suporte a mensagens multi-turn
- âœ… System prompt dedicado
- âœ… VersÃ£o da API configurÃ¡vel (padrÃ£o: `2023-06-01`)
- âœ… Stop sequences
- âœ… Streaming (via `parseAnthropicStream`)

**Endpoint:**
- `https://api.anthropic.com/v1`

**MÃ©todos principais:**
```typescript
async createMessage(params: MessagesParams)
```

**Headers obrigatÃ³rios:**
- `x-api-key`: Chave da API
- `anthropic-version`: VersÃ£o da API

---

### 3. Google GenAI Client (`/packages/google-genai/index.ts`)

**DescriÃ§Ã£o:** ImplementaÃ§Ã£o nativa para Google Gemini.

**CaracterÃ­sticas:**
- âœ… `generationConfig` com `temperature`, `topP`, `maxOutputTokens`
- âœ… `systemInstruction` para prompts de sistema
- âœ… Formato JSON via `responseMimeType: 'application/json'`
- âš ï¸ **Streaming nÃ£o implementado** (apenas texto e JSON)

**Endpoint:**
- `https://generativelanguage.googleapis.com/v1beta/models`

**MÃ©todos principais:**
```typescript
async generateContent(params: GenerateContentParams)
```

**ObservaÃ§Ã£o:**
- A API key Ã© passada como query parameter (`?key=${apiKey}`)

---

## ğŸ—ï¸ Arquitetura Unificada

Todas as 3 nLLMs sÃ£o utilizadas pelo **adaptador unificado** em `/src/adapters/run-llm.ts`:

```typescript
export async function runLLM(
  params: UnifiedLLMParams,
  output: "text" | "json" | "stream",
): Promise<UnifiedLLMTextResult & { json?: unknown; stream?: AsyncGenerator<string> }>
```

### Fluxo de DecisÃ£o

```
runLLM() 
  â”œâ”€ OpenAI Compatible? â†’ OpenAIClient
  â”‚   â”œâ”€ openai
  â”‚   â”œâ”€ groq
  â”‚   â”œâ”€ openrouter
  â”‚   â”œâ”€ deepseek
  â”‚   â”œâ”€ mistral
  â”‚   â””â”€ perplexity
  â”‚
  â”œâ”€ anthropic? â†’ AnthropicClient
  â”‚
  â””â”€ gemini? â†’ GoogleGenAIClient
```

---

## ğŸ§© DependÃªncias Nativas

O sistema utiliza a biblioteca **reqify** (`/packages/reqify`) como camada de HTTP nativa, evitando dependÃªncias externas como `axios`.

**MÃ©todos utilizados:**
- `reqify.post(url, data, options)` - RequisiÃ§Ãµes normais
- `reqify.postStream(url, data, options)` - RequisiÃ§Ãµes com streaming

---

## ğŸ“ Interface Fluente

As 3 nLLMs sÃ£o acessadas atravÃ©s da **Fluent Interface** em `/src/fluent/send-prompt.ts`:

```typescript
import { sendPrompt } from 'one-llm-4-all';

// Exemplo com OpenAI Client (via Groq)
const txt = await sendPrompt('Explique LLMs', { 
  model: 'llama-3.1-8b-instant', 
  provider: 'groq' 
}).getText();

// Exemplo com Anthropic Client
const response = await sendPrompt('Analise este cÃ³digo', { 
  model: 'claude-3-opus-20240229', 
  provider: 'anthropic' 
}).getText();

// Exemplo com Google GenAI Client
const json = await sendPrompt('Retorne JSON', { 
  model: 'gemini-pro', 
  provider: 'gemini' 
}).getJSONResponse();
```

---

## ğŸ¯ Provedores Suportados

| Provider | nLLM Utilizada | Streaming | JSON Mode |
|----------|----------------|-----------|-----------|
| `openai` | OpenAIClient | âœ… | âœ… |
| `groq` | OpenAIClient | âœ… | âœ… |
| `openrouter` | OpenAIClient | âœ… | âœ… |
| `deepseek` | OpenAIClient | âœ… | âœ… |
| `mistral` | OpenAIClient | âœ… | âœ… |
| `perplexity` | OpenAIClient | âœ… | âœ… |
| `anthropic` | AnthropicClient | âœ… | âœ… |
| `gemini` | GoogleGenAIClient | âŒ | âœ… |

**Total de provedores:** 8  
**Total de nLLMs:** 3

---

## ğŸ”§ UtilitÃ¡rios de Stream

Para suportar streaming, o sistema possui parsers dedicados em `/src/utils/stream-parser.ts`:

1. **`parseSSE(stream: ReadableStream)`** - Parser para Server-Sent Events (OpenAI compatible)
2. **`parseAnthropicStream(stream: ReadableStream)`** - Parser especÃ­fico para Anthropic

---

## ğŸ“¦ Estrutura de Tipos

Todos os tipos sÃ£o unificados em `/src/types/llm.types.ts` com **Tipagem SemÃ¢ntica Nominal**:

```typescript
export type Brand<K, T> = K & { __brand: T };

export type ApiKey = Brand<string, "ApiKey">;
export type ModelId = Brand<string, "ModelId">;
export type PromptContent = Brand<string, "PromptContent">;

export type LLMProvider = 
  | 'groq' 
  | 'openrouter' 
  | 'anthropic' 
  | 'gemini' 
  | 'openai' 
  | 'deepseek' 
  | 'mistral' 
  | 'perplexity';
```

---

## âœ… ConclusÃ£o

### Resposta Direta
**O sistema possui 3 (trÃªs) nLLMs:**
1. OpenAIClient
2. AnthropicClient
3. GoogleGenAIClient

### EstratÃ©gia Arquitetural
- **Sem dependÃªncias externas** de SDKs oficiais (OpenAI SDK, Anthropic SDK, etc.)
- **ImplementaÃ§Ã£o nativa** usando apenas `reqify` para HTTP
- **Interface unificada** que abstrai as diferenÃ§as entre provedores
- **Compatibilidade OpenAI** permite suportar 6 provedores com apenas 1 nLLM

### RecomendaÃ§Ãµes
1. âœ… Implementar streaming para Google GenAI Client
2. âœ… Adicionar testes unitÃ¡rios para cada nLLM
3. âœ… Documentar rate limits e error handling especÃ­ficos de cada provider
4. âœ… Considerar adicionar retry logic e exponential backoff

---

**Autor:** AI Assistant  
**RevisÃ£o:** Pendente
